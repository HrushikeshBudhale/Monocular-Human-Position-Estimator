\href{https://app.travis-ci.com/HrushikeshBudhale/Monocular-Human-Position-Estimator}{\texttt{ }} \href{https://coveralls.io/github/HrushikeshBudhale/Monocular-Human-Position-Estimator?branch=main}{\texttt{ }} \href{https://opensource.org/licenses/MIT}{\texttt{ }}\hypertarget{index_autotoc_md1}{}\doxysubsection{Project Overview and Description}\label{index_autotoc_md1}
Applications of Computer Vision are being rapidly deployed across diverse fields from simple object detection to more advanced self-\/driving vehicles. It is a field of artificial intelligence that enables computers and systems to derive meaningful information from digital images, videos, and other visual inputs and take actions or make recommendations based on that information. Acme Robotics is looking to leverage the field of Computer Vision by integrating a perception software module into its existing robotic hardware system to detect and track humans around a robot.

The scope of this project is to develop a software which detects and tracks one or more humans around a robot using a monocular camera. This is accomplished by using the pre-\/trained H\+OG (Histogram of Oriented Gradient) descriptor provided by Open\+CV. Along with this we also used an S\+VM (Support Vector Machine) detector from opencv to detect the location of humans in the camera frame. These features output the parameters of the bounding boxes around the detected humans. These parameters are then used to determine the 2d position (x,y) of the detected humans with respect to the camera. Using appropriate camera parameters and known values of the size of bounding boxes for an average human standing at a given distance from a camera, the value of depth (distance between the camera and human/s) is calculated by the method of equivalent ratio. These 3d-\/positions are then transformed to the robot’s frame to determine the position of humans with respect to the robot using an appropriate transformation matrix between the robot and camera frame. These 3d-\/positions are then fed into the tracker part of the software which assigns unique identity to the detected humans.

Detailed developer level documentation and installation steps have been created for new developers to contribute and use it with ease. This project ensures it’s correctness by testing every class on multiple unit tests created in the gtest suite. Latest build status and code coverage tests are performed in continuous integration with the help of travis and coverall. To ensure implementation accuracy each class has been developed by unit testing it on multiple test cases using the gtest suite.\hypertarget{index_autotoc_md2}{}\doxysubsection{Risks and Mitigation}\label{index_autotoc_md2}

\begin{DoxyEnumerate}
\item For detecting humans, H\+OG descriptor has been used which is not a state of the art detector and performs poorly in unevenly lit frames, it also fails to detect humans properly in poses other than standing pose like sitting, lying, bending positions. Detection performance can be improved by using new human detections techniques using neural networks.
\item Implemented tracker does not account for 2 humans crossing each other and can result in change the id assigned to humans on such events. Improved results can be obtained by properly estimating depth of each person from camera and using the knowledge from previous tracker output.
\item As the distance of human from camera is being calculated using monocular camera, the depth estimate of the tracker is too noisy, results can be improved by adding proper filter or by using depth sensor.
\end{DoxyEnumerate}

\DoxyHorRuler{0}
 \hypertarget{index_autotoc_md4}{}\doxysubsection{Product Backlog}\label{index_autotoc_md4}
Product backlog can be found in \href{https://docs.google.com/spreadsheets/d/1KF9aKQJTfanBHgDmTPipmk2IF8u1touHpT6VdUuIId4/edit?usp=sharing}{\texttt{ google sheet}}\hypertarget{index_autotoc_md5}{}\doxysubsection{Sprint planning notes}\label{index_autotoc_md5}
Sprint planning notes can found in \href{https://docs.google.com/document/d/1Ov0n0FiRoefbuTpbJCGnaQuJ2t1R9MHB2j_GyZQ0szY/edit?usp=sharing}{\texttt{ google docs}}\hypertarget{index_autotoc_md6}{}\doxysubsection{Deliverables}\label{index_autotoc_md6}

\begin{DoxyItemize}
\item Developer level documentation using doxygen
\item Build status using travis CI
\item Code coverage using coveralls
\item Accurate implementation using generated test cases in gtest
\item Steps showing how to build the repository
\item Steps showing how to run test and demo applications
\item Steps showing how to generate Doxygen documentation
\item Valgrind output for memory leak ceck
\end{DoxyItemize}\hypertarget{index_autotoc_md7}{}\doxysubsection{Documentation}\label{index_autotoc_md7}
Doxygen generated documentation can be found \href{https://hrushikeshbudhale.github.io/Monocular-Human-Position-Estimator/docs/html/index.html}{\texttt{ here}} ~\newline
 Run the following command in folder\textquotesingle{}s root directory to generate new documentation 
\begin{DoxyCode}{0}
\DoxyCodeLine{doxygen docs/doxygen\_config.conf}
\end{DoxyCode}
\hypertarget{index_autotoc_md8}{}\doxysubsection{Standard install via command-\/line}\label{index_autotoc_md8}

\begin{DoxyCode}{0}
\DoxyCodeLine{sudo apt-\/get update}
\DoxyCodeLine{sudo apt-\/get install libeigen3-\/dev}
\DoxyCodeLine{sudo apt-\/get install libopencv-\/dev}
\DoxyCodeLine{git clone -\/-\/recursive https://github.com/HrushikeshBudhale/Monocular-\/Human-\/Position-\/Estimator}
\DoxyCodeLine{cd Monocular-\/Human-\/Position-\/Estimator/}
\DoxyCodeLine{mkdir build}
\DoxyCodeLine{cd build}
\DoxyCodeLine{cmake ..}
\DoxyCodeLine{make}
\end{DoxyCode}


~\newline
\hypertarget{index_autotoc_md9}{}\doxysubsection{Building for code coverage}\label{index_autotoc_md9}

\begin{DoxyCode}{0}
\DoxyCodeLine{sudo apt-\/get install lcov}
\DoxyCodeLine{cmake -\/D COVERAGE=ON -\/D CMAKE\_BUILD\_TYPE=Debug ../}
\DoxyCodeLine{make}
\DoxyCodeLine{make code\_coverage}
\end{DoxyCode}


This generates a index.\+html page in the build/coverage sub-\/directory that can be viewed locally in a web browser.

~\newline
\hypertarget{index_autotoc_md10}{}\doxysubsection{Run main program}\label{index_autotoc_md10}
To run realtime tracking using camera, run following command 
\begin{DoxyCode}{0}
\DoxyCodeLine{./app/shell-\/app}
\end{DoxyCode}


To run tracking on video, run following command by providing file path to the existing video file 
\begin{DoxyCode}{0}
\DoxyCodeLine{./app/shell-\/app <file path>}
\end{DoxyCode}


~\newline
\hypertarget{index_autotoc_md11}{}\doxysubsection{Run tests}\label{index_autotoc_md11}

\begin{DoxyCode}{0}
\DoxyCodeLine{./test/cpp-\/test}
\end{DoxyCode}


~\newline
\hypertarget{index_autotoc_md12}{}\doxysubsection{cppcheck}\label{index_autotoc_md12}

\begin{DoxyCode}{0}
\DoxyCodeLine{find ./app ./include ./test -\/iname  *.cpp -\/or -\/iname *.hpp | xargs cppcheck}
\end{DoxyCode}


Existing results can be found in \href{https://github.com/HrushikeshBudhale/Monocular-Human-Position-Estimator/blob/main/results/cppcheck_output.txt}{\texttt{ /results/cppcheck\+\_\+output.txt}}

~\newline
\hypertarget{index_autotoc_md13}{}\doxysubsection{cpplint}\label{index_autotoc_md13}

\begin{DoxyCode}{0}
\DoxyCodeLine{find ./app ./include ./test -\/iname  *.cpp -\/or -\/iname *.hpp | xargs cpplint}
\end{DoxyCode}


Existing results can be found in \href{https://github.com/HrushikeshBudhale/Monocular-Human-Position-Estimator/blob/main/results/cpplint_output.txt}{\texttt{ /results/cpplint\+\_\+output.txt}}

~\newline
\hypertarget{index_autotoc_md14}{}\doxysubsection{Valgrind memory leak check}\label{index_autotoc_md14}
Checked for memory leaks uing valgrind. No memory leaks found. output can be found \href{https://github.com/HrushikeshBudhale/Monocular-Human-Position-Estimator/blob/main/results/valgrind_output.txt}{\texttt{ here}}

\DoxyHorRuler{0}
\hypertarget{index_autotoc_md16}{}\doxysubsection{U\+M\+L Class Diagram}\label{index_autotoc_md16}
 

\DoxyHorRuler{0}
\hypertarget{index_autotoc_md18}{}\doxysubsection{Activity Diagram}\label{index_autotoc_md18}
 

\DoxyHorRuler{0}
\hypertarget{index_autotoc_md20}{}\doxysubsection{Test Results}\label{index_autotoc_md20}
Performed tests on 15 different images. Out of them 10 images were of humans and remaining 5 were of other objects. \mbox{\hyperlink{classDetector}{Detector}} output showed 80\% accuracy with 2 false negatives and 1 false positive. \href{https://github.com/HrushikeshBudhale/Monocular-Human-Position-Estimator/blob/main/results/test_results\%20-\%20Sheet1.csv}{\texttt{ csv file}}\hypertarget{index_autotoc_md21}{}\doxysubsection{Video link}\label{index_autotoc_md21}

\begin{DoxyItemize}
\item \href{https://youtu.be/fk__0-6L_vQ}{\texttt{ Project Proposal Video}}
\item \href{https://youtu.be/0RSSTZF7N8k}{\texttt{ Phase1 Demo}}
\end{DoxyItemize}\hypertarget{index_autotoc_md22}{}\doxysubsection{External Dependencies}\label{index_autotoc_md22}

\begin{DoxyItemize}
\item \href{https://github.com/opencv/opencv}{\texttt{ Opencv}}
\item \href{https://eigen.tuxfamily.org/index.php?title=Main_Page}{\texttt{ Eigen3}} 
\end{DoxyItemize}